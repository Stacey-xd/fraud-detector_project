{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions Logistic Regression\n",
    "\n",
    "1. **General Performance**\n",
    "   - All logistic regression models achieved **very high accuracy (>97%)**, but this metric is misleading due to class imbalance.\n",
    "   - The key focus was on detecting **fraud cases** (minority class), which had only 95 instances in the test set.\n",
    "\n",
    "2. **Recall on Fraud Class**\n",
    "   - All models achieved **high recall** (≈0.87) for fraud, meaning they correctly identified most frauds.\n",
    "   - This came at the cost of **very low precision** (≈0.05–0.11), indicating many false positives.\n",
    "\n",
    "3. **Balanced SMOTE Models**\n",
    "   - Models like `logistic_regression_bsmote_eval.pkl` and its variations:\n",
    "     - Achieved **slightly higher precision** (up to 0.11) on the fraud class.\n",
    "     - F1-score was still low (max ≈0.20).\n",
    "     - ROC AUC varied between **0.927–0.944**, showing decent separation ability.\n",
    "\n",
    "4. **Cleaned-Only Models**\n",
    "   - Models without SMOTE (e.g. `logistic_regression_clean_eval_*.pkl`):\n",
    "     - Had **extremely consistent recall ≈0.8737** across all variants.\n",
    "     - Precision remained very low (~0.0529), suggesting limited improvement.\n",
    "     - Slight increase in ROC AUC (**up to 0.9690** in the best case).\n",
    "     - Many of these models were effectively **identical**, suggesting the optimization had converged.\n",
    "\n",
    "5. **Conclusion**\n",
    "   - Logistic regression is not suitable for fraud detection in this case.\n",
    "   - Despite its good recall, the **model is too often wrong**, labeling legitimate transactions as fraudulent.\n",
    "   - A F1-score of < 0.20** indicates low overall quality.\n",
    "   - The models are not ready for actual use in the banking environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions RandomForest\n",
    "1.\tThe model showed a very high overall accuracy (accuracy of about 0.9999) and a very good ability to detect fraud even with a modified threshold (threshold = 0.7), which allows for better control of the trade-off between false positives and false negatives.\n",
    "\n",
    "2.\tThe recall for the Fraud class reached 95.8%, which is a very high rate - the model detects almost all cases of fraud.\n",
    "\n",
    "3.\tThe F1-measure for Fraud was in the range of ~0.95-0.98, which indicates a balance between precision and recall, and a very good ability of the model to work with a difficult (less represented) class.\n",
    "\n",
    "4.\tROC AUC ~0.99+, which confirms the model's excellent ability to separate classes even at different probability thresholds.\n",
    "\n",
    "5.\tEasy retraining is possible, as some models had perfect metrics on the test set (e.g., 100% accuracy for the Legit class and almost 96% recall for Fraud). This may indicate that the model has partially adjusted to the data and should be tested on a completely independent dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions CatBoost & LightGBM Models\n",
    "\n",
    "1. **General Performance**\n",
    "   - Both CatBoost and LightGBM models achieved **very high accuracy (~99.96–99.98%)**, **excellent ROC AUC (>0.997)**, and strong fraud detection metrics.\n",
    "   - These results place them among the **top-performing models**, comparable to the best XGBoost variants.\n",
    "\n",
    "2. **CatBoost Tuned Model (`catboost_tuned_eval.pkl`)**\n",
    "   - Achieved **precision = 0.94**, **recall = 0.97**, and **F1-score = 0.95** on the fraud class.\n",
    "   - The model made only **3 false negatives** and **6 false positives** out of ~57k samples.\n",
    "   - ROC AUC = **0.9976**, indicating very strong discrimination capability.\n",
    "   - **Excellent balance** between catching fraud and minimizing false alarms.\n",
    "\n",
    "3. **LightGBM Tuned Model (`lightgbm_tuned_eval.pkl`)**\n",
    "   - Achieved **recall = 0.97**, but slightly lower **precision = 0.81** on fraud class.\n",
    "   - Produced more false positives (**21**) than CatBoost, which slightly lowered its F1-score (0.88).\n",
    "   - ROC AUC = **0.9980**, still among the highest.\n",
    "   - **Strong recall, moderate precision**, may be suitable where catching fraud is more critical than reducing false positives.\n",
    "\n",
    "4. **Conclusion**\n",
    "   - Both models are **highly effective** and can be used in production.\n",
    "   - **CatBoost** outperforms LightGBM in **precision and overall balance**, making it slightly more robust.\n",
    "   - **LightGBM** may still be preferred in cases requiring higher recall and model speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions Neural Network (NN)\n",
    "\n",
    "1. **General Performance**\n",
    "   - The tuned neural network model achieved **very high accuracy (~99.97%)**, close to the best tree-based models.\n",
    "   - However, as always, accuracy is **misleading** due to class imbalance.\n",
    "\n",
    "2. **Recall vs. Precision Trade-off**\n",
    "   - With the standard threshold, the model achieved **recall ≈0.79** and **precision ≈0.99** for fraud detection:\n",
    "     - This means the model **rarely makes false alarms** but still **misses some fraud cases**.\n",
    "   - When the threshold was lowered (e.g., 0.1), recall increased to **~0.89**, but precision dropped to **~0.19**:\n",
    "     - The model detected more fraud but **made many incorrect fraud predictions**.\n",
    "\n",
    "3. **F1-score and Balance**\n",
    "   - F1-score ranged from **~0.32 to ~0.88**, depending on the threshold.\n",
    "   - Indicates that the model can either:\n",
    "     - Be **very conservative** (few false positives, lower recall), or\n",
    "     - Be **aggressive** (high recall, low precision), depending on threshold tuning.\n",
    "\n",
    "4. **Conclusion**\n",
    "   - The neural network shows **potential**, but requires **careful threshold selection** depending on business priorities.\n",
    "   - While it does not outperform the best tuned XGBoost or CatBoost models, its results are **still solid**.\n",
    "   - May be useful in ensemble settings or where neural networks are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Logistic Regression** had good recall but very low precision — it marked too many normal transactions as fraud.  \n",
    "  ➤ Not suitable for real use.\n",
    "\n",
    "- **Random Forest** worked well but might be slightly overfitted.  \n",
    "  ➤ Needs more testing on new data.\n",
    "\n",
    "- **XGBoost** (with tuning or class weights) was the best:\n",
    "  - High precision and recall (≈96–100%)\n",
    "  - Some models made **zero** false alerts  \n",
    "  ➤ Ready for real use.\n",
    "\n",
    "- **CatBoost and LightGBM** also showed great results.\n",
    "  - **CatBoost** was more balanced.\n",
    "  - **LightGBM** found more fraud but made more mistakes.  \n",
    "  ➤ Both are strong choices.\n",
    "\n",
    "- **Neural Network (NN)** had good accuracy but was less reliable than boosted trees.  \n",
    "  ➤ Can be used together with other models.\n",
    "\n",
    "**Conclusion**:  \n",
    "**XGBoost and CatBoost are the best choices**. They are accurate, reliable, and safe to use for real fraud detection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
