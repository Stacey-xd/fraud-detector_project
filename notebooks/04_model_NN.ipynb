{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (PyTorch)\n",
    "\n",
    "- A **custom feedforward neural network** (`FraudNN`) was built using PyTorch, with:\n",
    "  - Two hidden layers (64 and 32 units)\n",
    "  - `ReLU` activations and a `Dropout` layer (p=0.3)\n",
    "  - A `Sigmoid` output for binary classification\n",
    "\n",
    "- The dataset was **standardized** using `StandardScaler`.\n",
    "\n",
    "- **Class imbalance** was addressed by passing a custom `pos_weight` to `BCELoss`, calculated as the ratio of legitimate to fraud cases.\n",
    "\n",
    "- The model was trained for **50 epochs** using the **Adam optimizer** with a learning rate of 0.001.\n",
    "\n",
    "- A **custom threshold of 0.7** was applied to the sigmoid output during evaluation to reduce false positives.\n",
    "\n",
    "- Final performance was measured using a confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 120.5741\n",
      "Epoch 2/200, Loss: 86.3876\n",
      "Epoch 3/200, Loss: 72.7200\n",
      "Epoch 4/200, Loss: 65.9608\n",
      "Epoch 5/200, Loss: 61.4218\n",
      "Epoch 6/200, Loss: 58.3176\n",
      "Epoch 7/200, Loss: 56.1627\n",
      "Epoch 8/200, Loss: 54.5278\n",
      "Epoch 9/200, Loss: 53.2985\n",
      "Epoch 10/200, Loss: 52.2352\n",
      "Epoch 11/200, Loss: 51.2572\n",
      "Epoch 12/200, Loss: 50.3834\n",
      "Epoch 13/200, Loss: 49.3685\n",
      "Epoch 14/200, Loss: 48.5354\n",
      "Epoch 15/200, Loss: 47.6297\n",
      "Epoch 16/200, Loss: 46.9201\n",
      "Epoch 17/200, Loss: 46.3157\n",
      "Epoch 18/200, Loss: 45.7583\n",
      "Epoch 19/200, Loss: 45.3787\n",
      "Epoch 20/200, Loss: 45.0509\n",
      "Epoch 21/200, Loss: 44.8148\n",
      "Epoch 22/200, Loss: 44.5398\n",
      "Epoch 23/200, Loss: 44.3951\n",
      "Epoch 24/200, Loss: 44.1865\n",
      "Epoch 25/200, Loss: 44.0802\n",
      "Epoch 26/200, Loss: 43.9777\n",
      "Epoch 27/200, Loss: 43.8595\n",
      "Epoch 28/200, Loss: 43.7905\n",
      "Epoch 29/200, Loss: 43.8246\n",
      "Epoch 30/200, Loss: 43.6651\n",
      "Epoch 31/200, Loss: 43.5809\n",
      "Epoch 32/200, Loss: 43.4714\n",
      "Epoch 33/200, Loss: 43.4535\n",
      "Epoch 34/200, Loss: 43.3542\n",
      "Epoch 35/200, Loss: 43.4051\n",
      "Epoch 36/200, Loss: 43.3448\n",
      "Epoch 37/200, Loss: 43.2261\n",
      "Epoch 38/200, Loss: 43.2260\n",
      "Epoch 39/200, Loss: 43.2150\n",
      "Epoch 40/200, Loss: 43.0976\n",
      "Epoch 41/200, Loss: 43.0897\n",
      "Epoch 42/200, Loss: 43.0507\n",
      "Epoch 43/200, Loss: 42.9896\n",
      "Epoch 44/200, Loss: 42.9615\n",
      "Epoch 45/200, Loss: 42.9400\n",
      "Epoch 46/200, Loss: 42.9067\n",
      "Epoch 47/200, Loss: 42.8635\n",
      "Epoch 48/200, Loss: 42.8417\n",
      "Epoch 49/200, Loss: 42.8198\n",
      "Epoch 50/200, Loss: 42.8011\n",
      "Epoch 51/200, Loss: 42.8818\n",
      "Epoch 52/200, Loss: 42.7445\n",
      "Epoch 53/200, Loss: 42.7207\n",
      "Epoch 54/200, Loss: 42.6569\n",
      "Epoch 55/200, Loss: 42.6686\n",
      "Epoch 56/200, Loss: 42.6351\n",
      "Epoch 57/200, Loss: 42.5711\n",
      "Epoch 58/200, Loss: 42.6081\n",
      "Epoch 59/200, Loss: 42.6058\n",
      "Epoch 60/200, Loss: 42.5088\n",
      "Epoch 61/200, Loss: 42.5837\n",
      "Epoch 62/200, Loss: 42.5052\n",
      "Epoch 63/200, Loss: 42.5943\n",
      "Epoch 64/200, Loss: 42.4304\n",
      "Epoch 65/200, Loss: 42.5021\n",
      "Epoch 66/200, Loss: 42.3968\n",
      "Epoch 67/200, Loss: 42.3559\n",
      "Epoch 68/200, Loss: 42.3933\n",
      "Epoch 69/200, Loss: 42.3390\n",
      "Epoch 70/200, Loss: 42.3327\n",
      "Epoch 71/200, Loss: 42.3380\n",
      "Epoch 72/200, Loss: 42.3348\n",
      "Epoch 73/200, Loss: 42.3046\n",
      "Epoch 74/200, Loss: 42.2549\n",
      "Epoch 75/200, Loss: 42.2694\n",
      "Epoch 76/200, Loss: 42.2617\n",
      "Epoch 77/200, Loss: 42.2575\n",
      "Epoch 78/200, Loss: 42.1846\n",
      "Epoch 79/200, Loss: 42.3134\n",
      "Epoch 80/200, Loss: 42.1719\n",
      "Epoch 81/200, Loss: 42.2109\n",
      "Epoch 82/200, Loss: 42.1959\n",
      "Epoch 83/200, Loss: 42.1876\n",
      "Epoch 84/200, Loss: 42.0900\n",
      "Epoch 85/200, Loss: 42.1490\n",
      "Epoch 86/200, Loss: 42.1097\n",
      "Epoch 87/200, Loss: 42.1681\n",
      "Epoch 88/200, Loss: 42.0693\n",
      "Epoch 89/200, Loss: 42.0140\n",
      "Epoch 90/200, Loss: 42.0314\n",
      "Epoch 91/200, Loss: 42.0145\n",
      "Epoch 92/200, Loss: 41.9912\n",
      "Epoch 93/200, Loss: 41.9619\n",
      "Epoch 94/200, Loss: 41.9612\n",
      "Epoch 95/200, Loss: 41.8881\n",
      "Epoch 96/200, Loss: 41.9071\n",
      "Epoch 97/200, Loss: 41.8674\n",
      "Epoch 98/200, Loss: 41.7021\n",
      "Epoch 99/200, Loss: 41.5294\n",
      "Epoch 100/200, Loss: 41.2801\n",
      "Epoch 101/200, Loss: 40.6793\n",
      "Epoch 102/200, Loss: 39.8771\n",
      "Epoch 103/200, Loss: 39.3618\n",
      "Epoch 104/200, Loss: 38.8615\n",
      "Epoch 105/200, Loss: 38.6992\n",
      "Epoch 106/200, Loss: 38.5058\n",
      "Epoch 107/200, Loss: 38.4250\n",
      "Epoch 108/200, Loss: 38.2785\n",
      "Epoch 109/200, Loss: 38.1931\n",
      "Epoch 110/200, Loss: 38.1885\n",
      "Epoch 111/200, Loss: 38.1262\n",
      "Epoch 112/200, Loss: 38.0754\n",
      "Epoch 113/200, Loss: 38.0110\n",
      "Epoch 114/200, Loss: 37.9845\n",
      "Epoch 115/200, Loss: 37.9315\n",
      "Epoch 116/200, Loss: 37.8983\n",
      "Epoch 117/200, Loss: 37.9085\n",
      "Epoch 118/200, Loss: 37.8557\n",
      "Epoch 119/200, Loss: 37.8390\n",
      "Epoch 120/200, Loss: 37.9076\n",
      "Epoch 121/200, Loss: 37.8272\n",
      "Epoch 122/200, Loss: 37.7883\n",
      "Epoch 123/200, Loss: 37.7760\n",
      "Epoch 124/200, Loss: 37.7683\n",
      "Epoch 125/200, Loss: 37.7362\n",
      "Epoch 126/200, Loss: 37.7214\n",
      "Epoch 127/200, Loss: 37.7419\n",
      "Epoch 128/200, Loss: 37.6653\n",
      "Epoch 129/200, Loss: 37.6527\n",
      "Epoch 130/200, Loss: 37.6314\n",
      "Epoch 131/200, Loss: 37.6177\n",
      "Epoch 132/200, Loss: 37.6436\n",
      "Epoch 133/200, Loss: 37.6341\n",
      "Epoch 134/200, Loss: 37.6420\n",
      "Epoch 135/200, Loss: 37.6025\n",
      "Epoch 136/200, Loss: 37.5802\n",
      "Epoch 137/200, Loss: 37.5889\n",
      "Epoch 138/200, Loss: 37.5742\n",
      "Epoch 139/200, Loss: 37.5504\n",
      "Epoch 140/200, Loss: 37.5301\n",
      "Epoch 141/200, Loss: 37.5750\n",
      "Epoch 142/200, Loss: 37.5484\n",
      "Epoch 143/200, Loss: 37.4991\n",
      "Epoch 144/200, Loss: 37.5228\n",
      "Epoch 145/200, Loss: 37.5366\n",
      "Epoch 146/200, Loss: 37.5510\n",
      "Epoch 147/200, Loss: 37.4984\n",
      "Epoch 148/200, Loss: 37.4904\n",
      "Epoch 149/200, Loss: 37.4871\n",
      "Epoch 150/200, Loss: 37.5110\n",
      "Epoch 151/200, Loss: 37.5174\n",
      "Epoch 152/200, Loss: 37.4581\n",
      "Epoch 153/200, Loss: 37.4327\n",
      "Epoch 154/200, Loss: 37.5253\n",
      "Epoch 155/200, Loss: 37.4585\n",
      "Epoch 156/200, Loss: 37.4843\n",
      "Epoch 157/200, Loss: 37.4148\n",
      "Epoch 158/200, Loss: 37.4540\n",
      "Epoch 159/200, Loss: 37.4413\n",
      "Epoch 160/200, Loss: 37.4430\n",
      "Epoch 161/200, Loss: 37.3971\n",
      "Epoch 162/200, Loss: 37.4291\n",
      "Epoch 163/200, Loss: 37.4748\n",
      "Epoch 164/200, Loss: 37.4043\n",
      "Epoch 165/200, Loss: 37.4202\n",
      "Epoch 166/200, Loss: 37.3811\n",
      "Epoch 167/200, Loss: 37.4308\n",
      "Epoch 168/200, Loss: 37.4099\n",
      "Epoch 169/200, Loss: 37.4445\n",
      "Epoch 170/200, Loss: 37.4520\n",
      "Epoch 171/200, Loss: 37.3727\n",
      "Epoch 172/200, Loss: 37.4070\n",
      "Epoch 173/200, Loss: 37.3755\n",
      "Epoch 174/200, Loss: 37.3382\n",
      "Epoch 175/200, Loss: 37.3988\n",
      "Epoch 176/200, Loss: 37.4702\n",
      "Epoch 177/200, Loss: 37.3886\n",
      "Epoch 178/200, Loss: 37.3802\n",
      "Epoch 179/200, Loss: 37.3308\n",
      "Epoch 180/200, Loss: 37.3441\n",
      "Epoch 181/200, Loss: 37.3361\n",
      "Epoch 182/200, Loss: 37.3811\n",
      "Epoch 183/200, Loss: 37.3313\n",
      "Epoch 184/200, Loss: 37.3060\n",
      "Epoch 185/200, Loss: 37.3049\n",
      "Epoch 186/200, Loss: 37.3452\n",
      "Epoch 187/200, Loss: 37.3190\n",
      "Epoch 188/200, Loss: 37.2967\n",
      "Epoch 189/200, Loss: 37.2997\n",
      "Epoch 190/200, Loss: 37.2826\n",
      "Epoch 191/200, Loss: 37.3454\n",
      "Epoch 192/200, Loss: 37.3010\n",
      "Epoch 193/200, Loss: 37.3198\n",
      "Epoch 194/200, Loss: 37.2939\n",
      "Epoch 195/200, Loss: 37.2625\n",
      "Epoch 196/200, Loss: 37.3260\n",
      "Epoch 197/200, Loss: 37.2663\n",
      "Epoch 198/200, Loss: 37.3806\n",
      "Epoch 199/200, Loss: 37.2720\n",
      "Epoch 200/200, Loss: 37.2687\n",
      "Confusion Matrix:\n",
      " [[56381   223]\n",
      " [   19    66]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Legit     0.9997    0.9961    0.9979     56604\n",
      "       Fraud     0.2284    0.7765    0.3529        85\n",
      "\n",
      "    accuracy                         0.9957     56689\n",
      "   macro avg     0.6140    0.8863    0.6754     56689\n",
      "weighted avg     0.9985    0.9957    0.9969     56689\n",
      "\n",
      "Model saved to models/fraud_nn_tuned_02.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv(\"data/creditcard_isoforest_cleaned_001.csv\")\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_legit = X_scaled[y == 0]\n",
    "\n",
    "# Split test set for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Torch datasets\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_legit, dtype=torch.float32)), batch_size=2048, shuffle=True)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.int)\n",
    "\n",
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cpu\")  # або \"cuda\" якщо доступний\n",
    "model = Autoencoder(X.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, x_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Compute reconstruction error\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(X_test_tensor.to(device))\n",
    "    mse = torch.mean((X_test_tensor - reconstructions.cpu()) ** 2, dim=1)\n",
    "\n",
    "    # Threshold (mean + 3*std from legit data)\n",
    "    recon_legit = model(torch.tensor(X_legit, dtype=torch.float32).to(device))\n",
    "    recon_mse = torch.mean((torch.tensor(X_legit, dtype=torch.float32) - recon_legit.cpu()) ** 2, dim=1)\n",
    "    threshold = recon_mse.mean() + 3 * recon_mse.std()\n",
    "\n",
    "# Classify\n",
    "y_pred = (mse > threshold).int()\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4, target_names=[\"Legit\", \"Fraud\"]))\n",
    "\n",
    "# Save model with auto-incremented filename\n",
    "import os\n",
    "\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "base_filename = \"fraud_nn_tuned\"\n",
    "ext = \".pt\"\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    filename = f\"{base_filename}{'' if i == 0 else f'_{i:02d}'}{ext}\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "torch.save(model.state_dict(), filepath)\n",
    "print(f\"Model saved to {filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model Update**  \n",
    "   - A **deeper Autoencoder** was implemented to better capture complex patterns in legitimate transaction data.  \n",
    "   - Architecture includes:\n",
    "     - Encoder: `input → 128 → 64 → 32 → 16`\n",
    "     - Decoder: `16 → 32 → 64 → 128 → input`\n",
    "\n",
    "2. **Training Configuration**  \n",
    "   - Trained on only legitimate transactions (`Class == 0`) using MSE loss.  \n",
    "   - Optimizer: Adam with a reduced learning rate (`1e-4`) for more stable convergence.  \n",
    "   - Epochs: 50\n",
    "\n",
    "3. **Reconstruction Threshold**  \n",
    "   - A new threshold was calculated using the **mean + 3×std** of reconstruction error on legit data (based on the deeper model’s output).  \n",
    "   - Used to classify anomalies on the full test set.\n",
    "\n",
    "4. **Model Saving**  \n",
    "   - The model was saved under an incremented name format to avoid overwriting earlier versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 123.1475\n",
      "Epoch 2/50, Loss: 94.8220\n",
      "Epoch 3/50, Loss: 81.7899\n",
      "Epoch 4/50, Loss: 72.6615\n",
      "Epoch 5/50, Loss: 67.4970\n",
      "Epoch 6/50, Loss: 64.3542\n",
      "Epoch 7/50, Loss: 61.9401\n",
      "Epoch 8/50, Loss: 59.6646\n",
      "Epoch 9/50, Loss: 57.8871\n",
      "Epoch 10/50, Loss: 56.4318\n",
      "Epoch 11/50, Loss: 55.3271\n",
      "Epoch 12/50, Loss: 54.3253\n",
      "Epoch 13/50, Loss: 53.3518\n",
      "Epoch 14/50, Loss: 52.5826\n",
      "Epoch 15/50, Loss: 51.8236\n",
      "Epoch 16/50, Loss: 51.1989\n",
      "Epoch 17/50, Loss: 50.5586\n",
      "Epoch 18/50, Loss: 49.8553\n",
      "Epoch 19/50, Loss: 49.3822\n",
      "Epoch 20/50, Loss: 48.7197\n",
      "Epoch 21/50, Loss: 48.2306\n",
      "Epoch 22/50, Loss: 47.5477\n",
      "Epoch 23/50, Loss: 46.8588\n",
      "Epoch 24/50, Loss: 46.1776\n",
      "Epoch 25/50, Loss: 45.5948\n",
      "Epoch 26/50, Loss: 45.0252\n",
      "Epoch 27/50, Loss: 44.5524\n",
      "Epoch 28/50, Loss: 44.3464\n",
      "Epoch 29/50, Loss: 43.8943\n",
      "Epoch 30/50, Loss: 43.5106\n",
      "Epoch 31/50, Loss: 43.2436\n",
      "Epoch 32/50, Loss: 43.0186\n",
      "Epoch 33/50, Loss: 42.6671\n",
      "Epoch 34/50, Loss: 42.3923\n",
      "Epoch 35/50, Loss: 42.2434\n",
      "Epoch 36/50, Loss: 41.9478\n",
      "Epoch 37/50, Loss: 41.6607\n",
      "Epoch 38/50, Loss: 41.4372\n",
      "Epoch 39/50, Loss: 41.3426\n",
      "Epoch 40/50, Loss: 41.0717\n",
      "Epoch 41/50, Loss: 40.8985\n",
      "Epoch 42/50, Loss: 40.5300\n",
      "Epoch 43/50, Loss: 40.4648\n",
      "Epoch 44/50, Loss: 40.2390\n",
      "Epoch 45/50, Loss: 39.8876\n",
      "Epoch 46/50, Loss: 39.5538\n",
      "Epoch 47/50, Loss: 39.4290\n",
      "Epoch 48/50, Loss: 39.0420\n",
      "Epoch 49/50, Loss: 38.8902\n",
      "Epoch 50/50, Loss: 38.6854\n",
      "Confusion Matrix:\n",
      " [[56344   260]\n",
      " [   18    67]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Legit     0.9997    0.9954    0.9975     56604\n",
      "       Fraud     0.2049    0.7882    0.3252        85\n",
      "\n",
      "    accuracy                         0.9951     56689\n",
      "   macro avg     0.6023    0.8918    0.6614     56689\n",
      "weighted avg     0.9985    0.9951    0.9965     56689\n",
      "\n",
      "Model saved to models/fraud_nn_tuned_03.pt\n"
     ]
    }
   ],
   "source": [
    "# Torch datasets\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_legit, dtype=torch.float32)), batch_size=2048, shuffle=True)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.int)\n",
    "\n",
    "# 4. Define a deeper Autoencoder\n",
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# Setup model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepAutoencoder(X.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, x_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Compute reconstruction error & threshold\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(X_test_tensor.to(device))\n",
    "    mse = torch.mean((X_test_tensor - reconstructions.cpu()) ** 2, dim=1)\n",
    "\n",
    "    # Compute threshold based on training (legit only)\n",
    "    recon_legit = model(torch.tensor(X_legit, dtype=torch.float32).to(device))\n",
    "    recon_mse = torch.mean((torch.tensor(X_legit, dtype=torch.float32) - recon_legit.cpu()) ** 2, dim=1)\n",
    "    threshold = recon_mse.mean() + 3 * recon_mse.std()\n",
    "\n",
    "# Predict\n",
    "y_pred = (mse > threshold).int()\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4, target_names=[\"Legit\", \"Fraud\"]))\n",
    "\n",
    "# Save model (auto-increment)\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "base_filename = \"fraud_nn_tuned\"\n",
    "ext = \".pt\"\n",
    "i = 1\n",
    "while True:\n",
    "    filename = f\"{base_filename}_{i:02d}{ext}\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "torch.save(model.state_dict(), filepath)\n",
    "print(f\"Model saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 47.1421\n",
      "Epoch 2/100, Loss: 39.2589\n",
      "Epoch 3/100, Loss: 34.5119\n",
      "Epoch 4/100, Loss: 30.9749\n",
      "Epoch 5/100, Loss: 28.0556\n",
      "Epoch 6/100, Loss: 25.6490\n",
      "Epoch 7/100, Loss: 23.5387\n",
      "Epoch 8/100, Loss: 21.7951\n",
      "Epoch 9/100, Loss: 20.2685\n",
      "Epoch 10/100, Loss: 18.9159\n",
      "Epoch 11/100, Loss: 17.6562\n",
      "Epoch 12/100, Loss: 16.4745\n",
      "Epoch 13/100, Loss: 15.5342\n",
      "Epoch 14/100, Loss: 14.7033\n",
      "Epoch 15/100, Loss: 13.9862\n",
      "Epoch 16/100, Loss: 13.3423\n",
      "Epoch 17/100, Loss: 12.7732\n",
      "Epoch 18/100, Loss: 12.2411\n",
      "Epoch 19/100, Loss: 11.7592\n",
      "Epoch 20/100, Loss: 11.3519\n",
      "Epoch 21/100, Loss: 10.9794\n",
      "Epoch 22/100, Loss: 10.6057\n",
      "Epoch 23/100, Loss: 10.2707\n",
      "Epoch 24/100, Loss: 9.9937\n",
      "Epoch 25/100, Loss: 9.7550\n",
      "Epoch 26/100, Loss: 9.5133\n",
      "Epoch 27/100, Loss: 9.2729\n",
      "Epoch 28/100, Loss: 9.0752\n",
      "Epoch 29/100, Loss: 8.8957\n",
      "Epoch 30/100, Loss: 8.6965\n",
      "Epoch 31/100, Loss: 8.5113\n",
      "Epoch 32/100, Loss: 8.3491\n",
      "Epoch 33/100, Loss: 8.2064\n",
      "Epoch 34/100, Loss: 8.0510\n",
      "Epoch 35/100, Loss: 7.9175\n",
      "Epoch 36/100, Loss: 7.7868\n",
      "Epoch 37/100, Loss: 7.6344\n",
      "Epoch 38/100, Loss: 7.5118\n",
      "Epoch 39/100, Loss: 7.3857\n",
      "Epoch 40/100, Loss: 7.2853\n",
      "Epoch 41/100, Loss: 7.1729\n",
      "Epoch 42/100, Loss: 7.0565\n",
      "Epoch 43/100, Loss: 6.9651\n",
      "Epoch 44/100, Loss: 6.8558\n",
      "Epoch 45/100, Loss: 6.7578\n",
      "Epoch 46/100, Loss: 6.6552\n",
      "Epoch 47/100, Loss: 6.5659\n",
      "Epoch 48/100, Loss: 6.4797\n",
      "Epoch 49/100, Loss: 6.3890\n",
      "Epoch 50/100, Loss: 6.2898\n",
      "Epoch 51/100, Loss: 6.2213\n",
      "Epoch 52/100, Loss: 6.1298\n",
      "Epoch 53/100, Loss: 6.0726\n",
      "Epoch 54/100, Loss: 5.9813\n",
      "Epoch 55/100, Loss: 5.9151\n",
      "Epoch 56/100, Loss: 5.8511\n",
      "Epoch 57/100, Loss: 5.7807\n",
      "Epoch 58/100, Loss: 5.7165\n",
      "Epoch 59/100, Loss: 5.6619\n",
      "Epoch 60/100, Loss: 5.6019\n",
      "Epoch 61/100, Loss: 5.5354\n",
      "Epoch 62/100, Loss: 5.4864\n",
      "Epoch 63/100, Loss: 5.4349\n",
      "Epoch 64/100, Loss: 5.3906\n",
      "Epoch 65/100, Loss: 5.3508\n",
      "Epoch 66/100, Loss: 5.2899\n",
      "Epoch 67/100, Loss: 5.2414\n",
      "Epoch 68/100, Loss: 5.1920\n",
      "Epoch 69/100, Loss: 5.1676\n",
      "Epoch 70/100, Loss: 5.1223\n",
      "Epoch 71/100, Loss: 5.0721\n",
      "Epoch 72/100, Loss: 5.0350\n",
      "Epoch 73/100, Loss: 5.0096\n",
      "Epoch 74/100, Loss: 4.9722\n",
      "Epoch 75/100, Loss: 4.9330\n",
      "Epoch 76/100, Loss: 4.9021\n",
      "Epoch 77/100, Loss: 4.8839\n",
      "Epoch 78/100, Loss: 4.8272\n",
      "Epoch 79/100, Loss: 4.7831\n",
      "Epoch 80/100, Loss: 4.7554\n",
      "Epoch 81/100, Loss: 4.7106\n",
      "Epoch 82/100, Loss: 4.7030\n",
      "Epoch 83/100, Loss: 4.6749\n",
      "Epoch 84/100, Loss: 4.6142\n",
      "Epoch 85/100, Loss: 4.6050\n",
      "Epoch 86/100, Loss: 4.5736\n",
      "Epoch 87/100, Loss: 4.5268\n",
      "Epoch 88/100, Loss: 4.5005\n",
      "Epoch 89/100, Loss: 4.4666\n",
      "Epoch 90/100, Loss: 4.4282\n",
      "Epoch 91/100, Loss: 4.3900\n",
      "Epoch 92/100, Loss: 4.3680\n",
      "Epoch 93/100, Loss: 4.3241\n",
      "Epoch 94/100, Loss: 4.2680\n",
      "Epoch 95/100, Loss: 4.2361\n",
      "Epoch 96/100, Loss: 4.1848\n",
      "Epoch 97/100, Loss: 4.1270\n",
      "Epoch 98/100, Loss: 4.0941\n",
      "Epoch 99/100, Loss: 4.0137\n",
      "Epoch 100/100, Loss: 3.9432\n",
      "Confusion Matrix:\n",
      " [[56363   241]\n",
      " [   19    66]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Legit     0.9997    0.9957    0.9977     56604\n",
      "       Fraud     0.2150    0.7765    0.3367        85\n",
      "\n",
      "    accuracy                         0.9954     56689\n",
      "   macro avg     0.6073    0.8861    0.6672     56689\n",
      "weighted avg     0.9985    0.9954    0.9967     56689\n",
      "\n",
      "Model saved to models/fraud_autoencoder_tuned_00.pt\n"
     ]
    }
   ],
   "source": [
    "# Torch datasets\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_legit, dtype=torch.float32)), batch_size=2048, shuffle=True)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.int)\n",
    "\n",
    "# Define the Autoencoder with BatchNorm\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Autoencoder(X.shape[1]).to(device)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, x_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Compute threshold from reconstruction error of legit\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(torch.tensor(X_legit, dtype=torch.float32).to(device))\n",
    "    recon_mse = torch.mean((torch.tensor(X_legit) - reconstructions.cpu())**2, dim=1)\n",
    "    threshold = recon_mse.mean() + 3 * recon_mse.std()\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    test_recon = model(X_test_tensor.to(device))\n",
    "    test_mse = torch.mean((X_test_tensor - test_recon.cpu()) ** 2, dim=1)\n",
    "    y_pred = (test_mse > threshold).int()\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4, target_names=[\"Legit\", \"Fraud\"]))\n",
    "\n",
    "# Save model\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "base_filename = \"fraud_autoencoder_tuned\"\n",
    "i = 0\n",
    "while True:\n",
    "    filename = f\"{base_filename}_{i:02d}.pt\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "torch.save(model.state_dict(), filepath)\n",
    "print(f\"Model saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "- **Autoencoder** with 3 layers in both the encoder and decoder.\n",
    "- **Batch Normalization** is applied after each layer.\n",
    "- Loss function: `SmoothL1Loss` (less sensitive to outliers).\n",
    "- Optimizer: `Adam`, learning rate = 0.0001\n",
    "- Epochs: 100, Batch size: 2048\n",
    "\n",
    "### Anomaly Detection\n",
    "- Reconstruction error (MSE) is computed using legitimate data only.\n",
    "- Threshold = mean + 3 * std of the reconstruction error.\n",
    "- If the error > threshold → classified as fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Loss: 0.5379\n",
      "Epoch 2/50 Loss: 1.4563\n",
      "Epoch 3/50 Loss: 0.2301\n",
      "Epoch 4/50 Loss: 0.3584\n",
      "Epoch 5/50 Loss: 2.2893\n",
      "Epoch 6/50 Loss: 0.1856\n",
      "Epoch 7/50 Loss: 0.2303\n",
      "Epoch 8/50 Loss: 0.4676\n",
      "Epoch 9/50 Loss: 0.0784\n",
      "Epoch 10/50 Loss: 0.4658\n",
      "Epoch 11/50 Loss: 0.1780\n",
      "Epoch 12/50 Loss: 0.2349\n",
      "Epoch 13/50 Loss: 0.5253\n",
      "Epoch 14/50 Loss: 0.6885\n",
      "Epoch 15/50 Loss: 0.1232\n",
      "Epoch 16/50 Loss: 0.3987\n",
      "Epoch 17/50 Loss: 0.2575\n",
      "Epoch 18/50 Loss: 0.1901\n",
      "Epoch 19/50 Loss: 0.1065\n",
      "Epoch 20/50 Loss: 0.2203\n",
      "Epoch 21/50 Loss: 0.3156\n",
      "Epoch 22/50 Loss: 0.0985\n",
      "Epoch 23/50 Loss: 0.0811\n",
      "Epoch 24/50 Loss: 0.1368\n",
      "Epoch 25/50 Loss: 0.6716\n",
      "Epoch 26/50 Loss: 22.5842\n",
      "Epoch 27/50 Loss: 0.5805\n",
      "Epoch 28/50 Loss: 0.5041\n",
      "Epoch 29/50 Loss: 0.2432\n",
      "Epoch 30/50 Loss: 0.0981\n",
      "Epoch 31/50 Loss: 0.9119\n",
      "Epoch 32/50 Loss: 0.0530\n",
      "Epoch 33/50 Loss: 0.8818\n",
      "Epoch 34/50 Loss: 0.9893\n",
      "Epoch 35/50 Loss: 0.5520\n",
      "Epoch 36/50 Loss: 3.5237\n",
      "Epoch 37/50 Loss: 0.0722\n",
      "Epoch 38/50 Loss: 3.2508\n",
      "Epoch 39/50 Loss: 0.1599\n",
      "Epoch 40/50 Loss: 0.2779\n",
      "Epoch 41/50 Loss: 0.1444\n",
      "Epoch 42/50 Loss: 0.3674\n",
      "Epoch 43/50 Loss: 5.4186\n",
      "Epoch 44/50 Loss: 0.0421\n",
      "Epoch 45/50 Loss: 0.4315\n",
      "Epoch 46/50 Loss: 0.1591\n",
      "Epoch 47/50 Loss: 2.4956\n",
      "Epoch 48/50 Loss: 2.9358\n",
      "Epoch 49/50 Loss: 1.0207\n",
      "Epoch 50/50 Loss: 0.2500\n",
      "Confusion Matrix:\n",
      " [[56603     1]\n",
      " [   18    67]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Legit     0.9997    1.0000    0.9998     56604\n",
      "       Fraud     0.9853    0.7882    0.8758        85\n",
      "\n",
      "    accuracy                         0.9997     56689\n",
      "   macro avg     0.9925    0.8941    0.9378     56689\n",
      "weighted avg     0.9997    0.9997    0.9996     56689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Dataset & Dataloader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Define model\n",
    "class FraudNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FraudNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FraudNN(X.shape[1])\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.BCELoss(weight=torch.tensor([((y == 0).sum() / (y == 1).sum())], dtype=torch.float32))  # pos_weight\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_prob = model(X_test_tensor).squeeze().numpy()\n",
    "    y_pred = (y_pred_prob >= 0.7).astype(int)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4, target_names=[\"Legit\", \"Fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model Improvements**\n",
    "   - Added two `Dropout(0.5)` layers (instead of 0.3) to reduce overfitting.\n",
    "   - Used `BCEWithLogitsLoss` instead of `BCELoss`, which is numerically more stable and avoids applying `Sigmoid` inside the model.\n",
    "\n",
    "2. **Class Imbalance Handling**\n",
    "   - Passed `pos_weight` directly to `BCEWithLogitsLoss` based on the ratio of legit to fraud cases in the training set.\n",
    "\n",
    "3. **Optimization**\n",
    "   - Replaced the standard `Adam` optimizer with `AdamW`, which adds better weight decay regularization (`weight_decay=1e-5`).\n",
    "\n",
    "4. **Batch Size**\n",
    "   - Increased the batch size to **2048** for more stable gradient estimates.\n",
    "\n",
    "5. **Early Stopping**\n",
    "   - Implemented early stopping with a **patience of 10 epochs**, saving the best model based on training loss.\n",
    "\n",
    "6. **Evaluation**\n",
    "   - Applied `sigmoid` manually during evaluation.\n",
    "   - Used a **custom threshold of 0.7** to reduce false positives.\n",
    "\n",
    "7. **Model Saving**\n",
    "   - Saved the final model weights as `fraud_nn_tuned.pt` in the `models/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Loss: 0.9239\n",
      "Epoch 2/100 Loss: 0.4933\n",
      "Epoch 3/100 Loss: 0.4035\n",
      "Epoch 4/100 Loss: 0.3976\n",
      "Epoch 5/100 Loss: 0.3603\n",
      "Epoch 6/100 Loss: 0.3368\n",
      "Epoch 7/100 Loss: 0.3422\n",
      "Epoch 8/100 Loss: 0.2868\n",
      "Epoch 9/100 Loss: 0.3063\n",
      "Epoch 10/100 Loss: 0.2801\n",
      "Epoch 11/100 Loss: 0.3071\n",
      "Epoch 12/100 Loss: 0.2841\n",
      "Epoch 13/100 Loss: 0.2808\n",
      "Epoch 14/100 Loss: 0.2680\n",
      "Epoch 15/100 Loss: 0.2892\n",
      "Epoch 16/100 Loss: 0.2571\n",
      "Epoch 17/100 Loss: 0.2551\n",
      "Epoch 18/100 Loss: 0.2266\n",
      "Epoch 19/100 Loss: 0.2561\n",
      "Epoch 20/100 Loss: 0.2241\n",
      "Epoch 21/100 Loss: 0.2171\n",
      "Epoch 22/100 Loss: 0.2303\n",
      "Epoch 23/100 Loss: 0.2234\n",
      "Epoch 24/100 Loss: 0.2134\n",
      "Epoch 25/100 Loss: 0.2229\n",
      "Epoch 26/100 Loss: 0.2277\n",
      "Epoch 27/100 Loss: 0.2094\n",
      "Epoch 28/100 Loss: 0.2020\n",
      "Epoch 29/100 Loss: 0.1955\n",
      "Epoch 30/100 Loss: 0.1803\n",
      "Epoch 31/100 Loss: 0.1886\n",
      "Epoch 32/100 Loss: 0.2216\n",
      "Epoch 33/100 Loss: 0.1918\n",
      "Epoch 34/100 Loss: 0.1587\n",
      "Epoch 35/100 Loss: 0.1901\n",
      "Epoch 36/100 Loss: 0.1689\n",
      "Epoch 37/100 Loss: 0.1713\n",
      "Epoch 38/100 Loss: 0.1532\n",
      "Epoch 39/100 Loss: 0.1479\n",
      "Epoch 40/100 Loss: 0.1574\n",
      "Epoch 41/100 Loss: 0.1655\n",
      "Epoch 42/100 Loss: 0.1573\n",
      "Epoch 43/100 Loss: 0.1561\n",
      "Epoch 44/100 Loss: 0.1459\n",
      "Epoch 45/100 Loss: 0.1610\n",
      "Epoch 46/100 Loss: 0.1332\n",
      "Epoch 47/100 Loss: 0.1421\n",
      "Epoch 48/100 Loss: 0.1435\n",
      "Epoch 49/100 Loss: 0.1444\n",
      "Epoch 50/100 Loss: 0.1330\n",
      "Epoch 51/100 Loss: 0.1444\n",
      "Epoch 52/100 Loss: 0.1399\n",
      "Epoch 53/100 Loss: 0.1276\n",
      "Epoch 54/100 Loss: 0.1359\n",
      "Epoch 55/100 Loss: 0.1290\n",
      "Epoch 56/100 Loss: 0.1312\n",
      "Epoch 57/100 Loss: 0.1285\n",
      "Epoch 58/100 Loss: 0.1222\n",
      "Epoch 59/100 Loss: 0.1202\n",
      "Epoch 60/100 Loss: 0.1197\n",
      "Epoch 61/100 Loss: 0.1161\n",
      "Epoch 62/100 Loss: 0.1423\n",
      "Epoch 63/100 Loss: 0.1072\n",
      "Epoch 64/100 Loss: 0.1065\n",
      "Epoch 65/100 Loss: 0.1165\n",
      "Epoch 66/100 Loss: 0.1192\n",
      "Epoch 67/100 Loss: 0.1420\n",
      "Epoch 68/100 Loss: 0.1010\n",
      "Epoch 69/100 Loss: 0.1114\n",
      "Epoch 70/100 Loss: 0.1307\n",
      "Epoch 71/100 Loss: 0.1106\n",
      "Epoch 72/100 Loss: 0.1019\n",
      "Epoch 73/100 Loss: 0.0978\n",
      "Epoch 74/100 Loss: 0.0955\n",
      "Epoch 75/100 Loss: 0.0961\n",
      "Epoch 76/100 Loss: 0.1246\n",
      "Epoch 77/100 Loss: 0.1045\n",
      "Epoch 78/100 Loss: 0.1012\n",
      "Epoch 79/100 Loss: 0.0887\n",
      "Epoch 80/100 Loss: 0.0880\n",
      "Epoch 81/100 Loss: 0.0916\n",
      "Epoch 82/100 Loss: 0.0820\n",
      "Epoch 83/100 Loss: 0.0893\n",
      "Epoch 84/100 Loss: 0.0940\n",
      "Epoch 85/100 Loss: 0.0867\n",
      "Epoch 86/100 Loss: 0.0947\n",
      "Epoch 87/100 Loss: 0.0918\n",
      "Epoch 88/100 Loss: 0.0837\n",
      "Epoch 89/100 Loss: 0.0962\n",
      "Epoch 90/100 Loss: 0.0862\n",
      "Epoch 91/100 Loss: 0.0892\n",
      "Epoch 92/100 Loss: 0.0921\n",
      "Early stopping triggered.\n",
      "Confusion Matrix:\n",
      " [[56286   318]\n",
      " [    9    76]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Legit     0.9998    0.9944    0.9971     56604\n",
      "       Fraud     0.1929    0.8941    0.3173        85\n",
      "\n",
      "    accuracy                         0.9942     56689\n",
      "   macro avg     0.5964    0.9442    0.6572     56689\n",
      "weighted avg     0.9986    0.9942    0.9961     56689\n",
      "\n",
      "Model saved to models/fraud_nn_tuned.pt\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=2048, shuffle=True)\n",
    "\n",
    "# Define model\n",
    "class FraudNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FraudNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 1)  # no sigmoid!\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = FraudNN(input_dim=X_train.shape[1])\n",
    "\n",
    "# Define loss with class imbalance\n",
    "pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()], dtype=torch.float32)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Train with early stopping\n",
    "EPOCHS = 100\n",
    "patience = 10\n",
    "min_loss = float('inf')\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_loss < min_loss:\n",
    "        min_loss = avg_loss\n",
    "        wait = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best weights\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_logits = model(X_test_tensor)\n",
    "    y_prob = torch.sigmoid(y_logits).numpy()\n",
    "    y_pred = (y_prob >= 0.7).astype(int)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=4, target_names=[\"Legit\", \"Fraud\"]))\n",
    "\n",
    "# Save model\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/fraud_nn_tuned.pt\")\n",
    "print(\"Model saved to models/fraud_nn_tuned.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder for Anomaly Detection (PyTorch)\n",
    "\n",
    "1. **Training on Legitimate Transactions Only**  \n",
    "   - The Autoencoder was trained **only on legit transactions** (`Class == 0`) to learn a clean reconstruction pattern.\n",
    "\n",
    "2. **Model Architecture**  \n",
    "   - Encoder: `input_dim → 20 → 10`  \n",
    "   - Decoder: `10 → 20 → input_dim`  \n",
    "   - Activation: ReLU between layers.\n",
    "\n",
    "3. **Training Setup**  \n",
    "   - Optimizer: Adam with `lr=1e-3`  \n",
    "   - Loss function: Mean Squared Error (MSE)  \n",
    "   - Trained for **200 epochs** on mini-batches of size **2048**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions Neural Network (NN)\n",
    "\n",
    "1. **General Performance**\n",
    "   - The tuned neural network model achieved **very high accuracy (~99.97%)**, close to the best tree-based models.\n",
    "   - However, as always, accuracy is **misleading** due to class imbalance.\n",
    "\n",
    "2. **Recall vs. Precision Trade-off**\n",
    "   - With the standard threshold, the model achieved **recall ≈0.79** and **precision ≈0.99** for fraud detection:\n",
    "     - This means the model **rarely makes false alarms** but still **misses some fraud cases**.\n",
    "   - When the threshold was lowered (e.g., 0.1), recall increased to **~0.89**, but precision dropped to **~0.19**:\n",
    "     - The model detected more fraud but **made many incorrect fraud predictions**.\n",
    "\n",
    "3. **F1-score and Balance**\n",
    "   - F1-score ranged from **~0.32 to ~0.88**, depending on the threshold.\n",
    "   - Indicates that the model can either:\n",
    "     - Be **very conservative** (few false positives, lower recall), or\n",
    "     - Be **aggressive** (high recall, low precision), depending on threshold tuning.\n",
    "\n",
    "4. **Conclusion**\n",
    "   - The neural network shows **potential**, but requires **careful threshold selection** depending on business priorities.\n",
    "   - While it does not outperform the best tuned XGBoost or CatBoost models, its results are **still solid**.\n",
    "   - May be useful in ensemble settings or where neural networks are preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
